{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_char_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPrp33CM1ZEk5FYhtZ1cB++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Black3rror/AI/blob/master/Playground/Keras_char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4tiRC6nVnm9"
      },
      "source": [
        "# Goal\n",
        "\n",
        "To implement RNN models in keras. We will use these models to learn the character level sequences.\n",
        "\n",
        "Tasks to do:\n",
        "- Vanilla RNN - done\n",
        "- LSTM RNN - done\n",
        "- Custom vanilla RNN - done\n",
        "- Custom LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tb1L0u6X61Z"
      },
      "source": [
        "# Importing stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzZ2y3nsVh5D"
      },
      "source": [
        "import numpy as np    # tf uses np so probabily we use np in our code\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG0R7xxEdbPr"
      },
      "source": [
        "# Custom functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0E8calndfJp"
      },
      "source": [
        "\"\"\"\n",
        "@param x: list of characters\n",
        "@return y: array of one-hot representation\n",
        "\"\"\"\n",
        "def char_to_hot(x, chars):\n",
        "  y = np.zeros((len(x), len(chars)))\n",
        "  char_to_indx = { ch:i for i,ch in enumerate(chars) }\n",
        "  for i in range(len(x)):\n",
        "    y[i, char_to_indx[x[i]]] = 1\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAEZFsjFCBBU"
      },
      "source": [
        "\"\"\"\n",
        "@param x: array of one-hot representation\n",
        "@return y: list of characters\n",
        "\"\"\"\n",
        "def hot_to_char(x, chars):\n",
        "  indx_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "  indxs = np.argmax(x, axis = 1)\n",
        "  y = []\n",
        "  for i in range(len(x)):\n",
        "    y.append(indx_to_char[indxs[i]])\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhtolrFhuhUn"
      },
      "source": [
        "def gen_text(rnn_gen, rnn_state, first_char_hot, len):\n",
        "  h = rnn_state\n",
        "  new_text = [first_char_hot]\n",
        "\n",
        "  for _ in range(len):\n",
        "    x, h = rnn_gen(new_text[-1].reshape(1, 1, c), initial_state = h)\n",
        "    pred_p = dense(x)\n",
        "\n",
        "    pred = np.zeros_like(pred_p)\n",
        "    for m, n in enumerate(np.argmax(pred_p, axis=1)):\n",
        "      pred[m, n] = 1\n",
        "\n",
        "    new_text = np.concatenate((new_text, pred))\n",
        "\n",
        "  new_text = hot_to_char(new_text, chars)\n",
        "  new_text_str = \"\"\n",
        "  new_text_str = new_text_str.join(new_text)\n",
        "  return new_text_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huHKgeqG1zZW"
      },
      "source": [
        "# Custom classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsHtltil2Ej0"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1vSjLEn13Dk"
      },
      "source": [
        "### SimpleRNN_custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4dA9qjR2Huq"
      },
      "source": [
        "class SimpleRNN_custom(layers.Layer):\n",
        "  def __init__(self, units, return_state = False, return_sequences = False, \n",
        "               keep_state = False, kernel_initializer = 'glorot_uniform', \n",
        "               recurrent_initializer = 'orthogonal', **kwargs):\n",
        "    super(SimpleRNN_custom, self).__init__(**kwargs)\n",
        "    self.units = units\n",
        "    self.return_state = return_state\n",
        "    self.return_sequences = return_sequences\n",
        "    self.keep_state = keep_state\n",
        "    self.kernel_initializer = kernel_initializer\n",
        "    self.recurrent_initializer = recurrent_initializer\n",
        "    self.h = None\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    assert len(input_shape) == 3    # ([batch_size, ]seq_len, prev_units)\n",
        "    self.Wxh = self.add_weight(name='Wxh',\n",
        "                               shape=(input_shape[2], self.units),\n",
        "                               initializer=self.kernel_initializer,\n",
        "                               trainable=True)\n",
        "    self.Whh = self.add_weight(name='Whh',\n",
        "                               shape=(self.units, self.units),\n",
        "                               initializer=self.recurrent_initializer,\n",
        "                               trainable=True)\n",
        "    self.b = self.add_weight(name='b',\n",
        "                             shape=(self.units, 1),\n",
        "                             initializer='zeros',\n",
        "                             trainable=True)\n",
        "  \n",
        "  def call(self, inputs, initial_state=None):\n",
        "    assert inputs.ndim == 3       # (batch_size, seq_len, prev_units)\n",
        "    if initial_state is not None:\n",
        "      assert initial_state.shape == [inputs.shape[0], self.units]\n",
        "      self.h = initial_state\n",
        "    elif self.keep_state == True and self.h is not None:\n",
        "      assert self.h.shape[0] == inputs.shape[0]\n",
        "    else:\n",
        "      self.h = tf.zeros((inputs.shape[0], self.units))\n",
        "    \n",
        "    hs = []\n",
        "    for i in range(inputs.shape[1]):\n",
        "      x = inputs[:, i, :]\n",
        "      h = tf.matmul(x, self.Wxh) + tf.matmul(self.h, self.Whh) + tf.transpose(self.b)\n",
        "      h = tf.tanh(h)\n",
        "\n",
        "      hs.append(h)\n",
        "      self.h = h\n",
        "    hs = tf.reshape(tf.convert_to_tensor(hs), (inputs.shape[0], inputs.shape[1], -1))\n",
        "\n",
        "    if self.return_sequences == True:\n",
        "      if self.return_state == True:\n",
        "        return hs, self.h\n",
        "      else:\n",
        "        return hs\n",
        "    else:\n",
        "      if self.return_state == True:\n",
        "        return self.h, self.h\n",
        "      else:\n",
        "        return self.h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OOrAOyEYlZ4"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9Le8g_pYnN_",
        "outputId": "7b05f341-bf65-4abb-9951-be74aadea041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "epochs = 3\n",
        "seq_len = 25\n",
        "h_units = 100\n",
        "learning_rate = 1e-1\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "text_adrs = '/content/drive/My Drive/Colab Stuff/Mini_char_RNN/William Shakespear.txt'\n",
        "\n",
        "text = open(text_adrs, 'r').read()\n",
        "text = text[:100000]\n",
        "chars = sorted(list(set(text)))\n",
        "c = len(chars)\n",
        "print(\"text has %d characters, %d unique.\" % (len(text), c))\n",
        "\n",
        "text = char_to_hot(text, chars)\n",
        "print(\"text shape: \", text.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "text has 100000 characters, 78 unique.\n",
            "text shape:  (100000, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4JZneA7-08"
      },
      "source": [
        "# No model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWwBaDWxUMXq"
      },
      "source": [
        "We will give SimpleRNN layer a batch with len 1 (input shape is (1, seq_len, prev_units)) but in fact it has seq_len examples and it will change to a batch with seq_len examples before going to Dense layer.\n",
        "\n",
        "We use rnn_gen which has input shape of (1, 1, prev_units) and is used for generating new text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WroyljdW8C7_",
        "outputId": "705f65c8-6ba8-46d5-ea47-1abf9d39cc12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_func = keras.losses.CategoricalCrossentropy()\n",
        "opt = keras.optimizers.Adagrad(learning_rate)\n",
        "\n",
        "# layers\n",
        "rnn = SimpleRNN_custom(h_units, kernel_initializer = 'random_normal', \n",
        "                       recurrent_initializer = 'random_normal', \n",
        "                       return_state = True, \n",
        "                       return_sequences = True, \n",
        "                       input_shape=(seq_len, c))\n",
        "\n",
        "rnn_gen = SimpleRNN_custom(h_units, kernel_initializer = 'random_normal', \n",
        "                           recurrent_initializer = 'random_normal', \n",
        "                           return_state = True, \n",
        "                           input_shape=(1, c))\n",
        "\n",
        "dense = layers.Dense(c, activation='softmax', \n",
        "                     kernel_initializer = 'random_normal')\n",
        "\n",
        "rnn_gen(text[0].reshape(1, 1, c))   # call it to build it\n",
        "\n",
        "smooth_loss = 0\n",
        "for epoch_num in range(epochs):\n",
        "  print(\"epoch %d started -------------\" % (epoch_num))\n",
        "  state = None\n",
        "\n",
        "  for step, pointer in enumerate(range(0, len(text) - seq_len - 1, seq_len)):\n",
        "    X_batch = text[pointer:pointer+seq_len].reshape(1, seq_len, c)\n",
        "    y_batch = text[pointer+1:pointer+seq_len+1].reshape(1, seq_len, c)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      if state is not None:\n",
        "        x, state = rnn(X_batch, initial_state = state)\n",
        "      else:\n",
        "        x, state = rnn(X_batch)\n",
        "      x = dense(x)\n",
        "      loss = loss_func(y_batch, x)\n",
        "\n",
        "    trainable_vars = rnn.trainable_weights + dense.trainable_weights\n",
        "    grads = tape.gradient(loss, trainable_vars)\n",
        "    opt.apply_gradients(zip(grads, trainable_vars))\n",
        "\n",
        "    smooth_loss = 0.999 * smooth_loss + 0.001 * loss\n",
        "    if step == 0 and epoch_num == 0:\n",
        "      smooth_loss = loss\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      print(\"Step \", step, \":\\t loss = \", smooth_loss.numpy())\n",
        "      \n",
        "      sp = int(200 * np.random.rand())\n",
        "      rnn_gen.set_weights(rnn.get_weights())\n",
        "      new_text_str = gen_text(rnn_gen, state, text[sp], 100)\n",
        "      print(\"new text: \", new_text_str)\n",
        "      print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer simple_rnn_custom_37 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "epoch 0 started -------------\n",
            "WARNING:tensorflow:Layer simple_rnn_custom_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step  0 :\t loss =  4.353026\n",
            "new text:   e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
            "\n",
            "\n",
            "\n",
            "Step  200 :\t loss =  4.116248\n",
            "new text:  i                                                                                                    \n",
            "\n",
            "\n",
            "\n",
            "Step  400 :\t loss =  3.8870087\n",
            "new text:                                                                                                       \n",
            "\n",
            "\n",
            "\n",
            "Step  600 :\t loss =  3.668711\n",
            "new text:   ou the the the the the the the the the the the the the the the the the the the the the the the the t\n",
            "\n",
            "\n",
            "\n",
            "Step  800 :\t loss =  3.4622006\n",
            "new text:  â€™s  he more then the then the then the then the then the then the then the then the then the then the\n",
            "\n",
            "\n",
            "\n",
            "Step  1000 :\t loss =  3.2758377\n",
            "new text:  y the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "\n",
            "\n",
            "Step  1200 :\t loss =  3.0966196\n",
            "new text:  ath thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee th\n",
            "\n",
            "\n",
            "\n",
            "Step  1400 :\t loss =  2.9451594\n",
            "new text:  h th                                                                                                 \n",
            "\n",
            "\n",
            "\n",
            "Step  1600 :\t loss =  2.8269424\n",
            "new text:  s ar your tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour t\n",
            "\n",
            "\n",
            "\n",
            "Step  1800 :\t loss =  2.7270079\n",
            "new text:   ting ting ting ting ting ting ting ting ting ting ting ting ting ting ting ting ting ting ting ting \n",
            "\n",
            "\n",
            "\n",
            "Step  2000 :\t loss =  2.618571\n",
            "new text:  ith the the the the the the the the the the the the the the the the the the the the the the the the t\n",
            "\n",
            "\n",
            "\n",
            "Step  2200 :\t loss =  2.537386\n",
            "new text:  y dove the ther all the ther all the ther all the ther all the ther all the ther all the ther all the\n",
            "\n",
            "\n",
            "\n",
            "Step  2400 :\t loss =  2.468606\n",
            "new text:  ror ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar ar a\n",
            "\n",
            "\n",
            "\n",
            "Step  2600 :\t loss =  2.4033365\n",
            "new text:  tee shee sing and the sered and the sered and the sered and the sered and the sered and the sered and\n",
            "\n",
            "\n",
            "\n",
            "Step  2800 :\t loss =  2.3594103\n",
            "new text:  he, and thee, and thee, and thee, and thee, and thee, and thee, and thee, and thee, and thee, and the\n",
            "\n",
            "\n",
            "\n",
            "Step  3000 :\t loss =  2.3156338\n",
            "new text:  n the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "\n",
            "\n",
            "Step  3200 :\t loss =  2.2865224\n",
            "new text:  t  hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I hat I ha\n",
            "\n",
            "\n",
            "\n",
            "Step  3400 :\t loss =  2.2597008\n",
            "new text:  thas and and and and and and and and and and and and and and and and and and and and and and and and \n",
            "\n",
            "\n",
            "\n",
            "Step  3600 :\t loss =  2.2078075\n",
            "new text:  d the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "\n",
            "\n",
            "Step  3800 :\t loss =  2.1761842\n",
            "new text:  th the hat the hat the hat the hat the hat the hat the hat the hat the hat the hat the hat the hat th\n",
            "\n",
            "\n",
            "\n",
            "epoch 1 started -------------\n",
            "Step  0 :\t loss =  2.1508594\n",
            "new text:  and the fore the fored the fore the fored the fore the fored the fore the fored the fore the fored th\n",
            "\n",
            "\n",
            "\n",
            "Step  200 :\t loss =  2.1603682\n",
            "new text:  \n",
            "\n",
            "                                                                                                   \n",
            "\n",
            "\n",
            "\n",
            "Step  400 :\t loss =  2.13774\n",
            "new text:   the the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "\n",
            "\n",
            "\n",
            "Step  600 :\t loss =  2.1210542\n",
            "new text:  love the with the with the with the with the with the with the with the with the with the with the wi\n",
            "\n",
            "\n",
            "\n",
            "Step  800 :\t loss =  2.1004865\n",
            "new text:  men the sear my shell the searth the sear my shell the searth the sear my shell the searth the sear m\n",
            "\n",
            "\n",
            "\n",
            "Step  1000 :\t loss =  2.0822036\n",
            "new text:  eave the stare the stare the stare the stare the stare the stare the stare the stare the stare the st\n",
            "\n",
            "\n",
            "\n",
            "Step  1200 :\t loss =  2.0482647\n",
            "new text:   thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee \n",
            "\n",
            "\n",
            "\n",
            "Step  1400 :\t loss =  2.0274136\n",
            "new text:  our the stare the star stor my sear my sear my sear my sear my sear my sear my sear my sear my sear m\n",
            "\n",
            "\n",
            "\n",
            "Step  1600 :\t loss =  2.0236528\n",
            "new text:  Wher to the store,\n",
            "That your to the store,\n",
            "That your to the store,\n",
            "That your to the store,\n",
            "That your \n",
            "\n",
            "\n",
            "\n",
            "Step  1800 :\t loss =  2.0210838\n",
            "new text:  o the thath the with simed in the stall stall stall stall stall stall stall stall stall stall stall s\n",
            "\n",
            "\n",
            "\n",
            "Step  2000 :\t loss =  1.9978096\n",
            "new text:  te the the the the the the the the the the the the the the the the the the the the the the the the th\n",
            "\n",
            "\n",
            "\n",
            "Step  2200 :\t loss =  1.9878844\n",
            "new text:  at the worth shell the worth shell the worth shell the worth shell the worth shell the worth shell th\n",
            "\n",
            "\n",
            "\n",
            "Step  2400 :\t loss =  1.9765142\n",
            "new text:  sered an the wort the wort the wort the wort the wort the wort the wort the wort the wort the wort th\n",
            "\n",
            "\n",
            "\n",
            "Step  2600 :\t loss =  1.9624456\n",
            "new text:  Thes are the stere,\n",
            "And the stare,\n",
            "And the stare,\n",
            "And the stare,\n",
            "And the stare,\n",
            "And the stare,\n",
            "And th\n",
            "\n",
            "\n",
            "\n",
            "Step  2800 :\t loss =  1.9639289\n",
            "new text:  and thee wind, and thee wind, and thee wind, and thee wind, and thee wind, and thee wind, and thee wi\n",
            "\n",
            "\n",
            "\n",
            "Step  3000 :\t loss =  1.958427\n",
            "new text:  gur steet me the store,\n",
            "The me the with the store,\n",
            "The me the with the store,\n",
            "The me the with the sto\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-39c6c8dae397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtrainable_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# This does not work with v1 TensorArrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   if ops.executing_eagerly_outside_functions(\n\u001b[0m\u001b[1;32m    157\u001b[0m   ) or control_flow_util.EnableControlFlowV2(ops.get_default_graph()):\n\u001b[1;32m    158\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mexecuting_eagerly_outside_functions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5767\u001b[0m     \u001b[0mboolean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutermost\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meager\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5768\u001b[0m   \"\"\"\n\u001b[0;32m-> 5769\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5770\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5771\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mthread\u001b[0m \u001b[0mhas\u001b[0m \u001b[0meager\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m   \"\"\"\n\u001b[0;32m-> 1905\u001b[0;31m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1906\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_execution_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcontext_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcontext_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m   \u001b[0;34m\"\"\"Returns current context (or None if one hasn't been initialized).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}