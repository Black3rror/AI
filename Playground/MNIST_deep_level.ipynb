{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST deep level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNaRWvviwArPc/za8lSUzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Black3rror/AI/blob/master/MNIST_deep_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQCI0-P3IZ6Q",
        "colab_type": "text"
      },
      "source": [
        "# Goal\n",
        "\n",
        "- Make custom layers - done\n",
        "- Make custom model - done\n",
        "- Make custom loss function\n",
        "- Using GradientTape to compute gradients - done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCPo4__KVDYp",
        "colab_type": "text"
      },
      "source": [
        "# Look underhood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGrSMC_IUj-C",
        "colab_type": "text"
      },
      "source": [
        "All operations are under supervision of TensorFlow, so it can calculate the gradients of all we do (when executing operation with GradientTape). In other word it can calculate gradient of anything (loss value) with respect to anything (all the trainable variables of the model).\n",
        "\n",
        "So to introduce a layer we just need to introduce its trainable parameters (weights and biases) and how the output of the layer is computed.\n",
        "Then the model is nothing but specifying the flow of input between these layers.\n",
        "\n",
        "Loss function is just another bunch of operations done after the model outputed. Of course this loss can be a part of a bigger loss which includes regularization loss. Regularization loss can be simply another bunch of operations which has been done in the layers (e.g. L2 of weights) and will be added to the final loss function.\n",
        "\n",
        "After that gradients with respect to all of the trainable parameters has been calculated, optimizer says that how to use these gradients to change the trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGGBx4tCJXrU",
        "colab_type": "text"
      },
      "source": [
        "# Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3agRmPdcrDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np    # tf uses np so probabily we use np in our code\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Layer, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, Softmax\n",
        "\n",
        "import matplotlib.pyplot as plt   # if u want to show imgs by pyplot\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime   # to organize TensorBoard files\n",
        "\n",
        "from keras.utils import to_categorical    # to change a number to one-hot key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiWYNp1ExYrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/   # Clear any logs from previous runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d388hJ2LoE2",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBRbrQPLsum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#assert len(tf.config.list_physical_devices('GPU')) > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exo-RHr6L0sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "0d7ddf35-822c-4fac-9021-9f2d44b15779"
      },
      "source": [
        "(trainX, trainy_int), (testX, testy_int) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"trainX_shape: \", trainX.shape)\n",
        "print(\"trainy_shape: \", trainy_int.shape)\n",
        "\n",
        "plt.imshow(trainX[0])\n",
        "print(\"y related to the shown image: \", trainy_int[0])\n",
        "\n",
        "# one-hot\n",
        "trainy = to_categorical(trainy_int)\n",
        "testy = to_categorical(testy_int)\n",
        "\n",
        "# image should be in shape of (28, 28, 1) not (28, 28)\n",
        "trainX = np.expand_dims(trainX, -1)\n",
        "testX = np.expand_dims(testX, -1)\n",
        "\n",
        "# normalize\n",
        "trainX = trainX.astype(\"float32\")/255\n",
        "testX = testX.astype(\"float32\")/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "trainX_shape:  (60000, 28, 28)\n",
            "trainy_shape:  (60000,)\n",
            "y related to the shown image:  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Wu76Y7KUFK",
        "colab_type": "text"
      },
      "source": [
        "# Make custom classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDw6aPLKcQr",
        "colab_type": "text"
      },
      "source": [
        "## Custom layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLyIHi7Khw5",
        "colab_type": "text"
      },
      "source": [
        "### Dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuoThG_sKk2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense_custom(Layer):\n",
        "  def __init__(self, units, activation=None, **kwargs):\n",
        "    super(Dense_custom, self).__init__(**kwargs)\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(name=\"weights\", shape=(input_shape[1], self.units), \n",
        "                             initializer=\"glorot_uniform\", trainable=True)\n",
        "    self.b = self.add_weight(name=\"biases\", shape=(self.units, ), \n",
        "                             initializer=\"zeros\", trainable=True)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    if self.activation == None:\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "    else:\n",
        "      z = tf.matmul(inputs, self.w) + self.b\n",
        "      act_layer = Activation(self.activation)\n",
        "      return act_layer(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNIqESulqTCS",
        "colab_type": "text"
      },
      "source": [
        "### Conv2D_custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtfe6V6wqfQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv2D_custom(Layer):\n",
        "  def __init__(self, filters, kernel_size, strides=(1, 1), padding=\"VALID\", activation=None, **kwargs):\n",
        "    super(Conv2D_custom, self).__init__(**kwargs)\n",
        "    self.filters_count = filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.strides = strides\n",
        "    self.padding = padding\n",
        "    self.activation = activation\n",
        "  \n",
        "  def build(self, input_shape):   # input_shape = [None (batch_size), img_height, img_width, img_depth]\n",
        "    self.w = self.add_weight(name=\"weights\", shape=(self.kernel_size[0], self.kernel_size[1], input_shape[3], self.filters_count), \n",
        "                             initializer=\"glorot_uniform\", trainable=True)    # shape = (kernel_h, kernel_w, last_layer_d, self_layer_d)\n",
        "    self.b = self.add_weight(name=\"biases\", shape=(self.filters_count, ), \n",
        "                             initializer=\"zeros\", trainable=True)\n",
        "  \n",
        "  def call(self, inputs):   # inputs shape: (batch_size (None), img_height, img_width, img_depth)\n",
        "    if self.activation == None:\n",
        "      return tf.nn.conv2d(inputs, self.w, self.strides, self.padding) + self.b\n",
        "    else:\n",
        "      z = tf.nn.conv2d(inputs, self.w, self.strides, self.padding) + self.b\n",
        "      act_layer = Activation(self.activation)\n",
        "      return act_layer(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCcPCcg55Tau",
        "colab_type": "text"
      },
      "source": [
        "## Custom model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OnRzFD15XR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bug: you can't call summary() before giving an input to the model or calling \n",
        "# fit(), even if you declared the input_shape\n",
        "class Sequential_custom(Model):\n",
        "  def __init__(self, layers=None, name=None):\n",
        "    super(Sequential_custom, self).__init__()\n",
        "    if layers == None:\n",
        "      self.seq_layers = []\n",
        "    else:\n",
        "      self.seq_layers = layers\n",
        "    self.model_name = name\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.seq_layers.append(layer)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    for layer in self.seq_layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpLj2kR5Tss4",
        "colab_type": "text"
      },
      "source": [
        "# Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO59Dq7IUTxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(conv1_units = 32, conv2_units = 64, dropout_rate = 0.5):\n",
        "  #model = Sequential()\n",
        "  model = Sequential_custom()\n",
        "\n",
        "  #model.add(Conv2D(conv1_units, (3,3), activation='relu', input_shape = trainX.shape[1:]))\n",
        "  model.add(Conv2D_custom(conv1_units, (3,3), activation='relu', input_shape = trainX.shape[1:]))\n",
        "\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  #model.add(Conv2D(conv2_units, (3,3), activation='relu'))\n",
        "  model.add(Conv2D_custom(conv2_units, (3,3), activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  \n",
        "  #model.add(Dense(10, activation='softmax'))\n",
        "  model.add(Dense_custom(10, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gT0R9hDYAUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2cY-A2t8H7",
        "colab_type": "text"
      },
      "source": [
        "# Compile and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcaV3Qw_t-8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1GiZF9529Na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir logs/fit    # to run the TensorBoard in the notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hMKiW-lvpCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "3dbd07af-93e3-4be8-e608-268956d701b9"
      },
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model.fit(trainX, trainy, epochs=10, validation_split=0.1, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   2/1688 [..............................] - ETA: 1:04 - loss: 2.3331 - categorical_accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 0.0673s). Check your callbacks.\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.2249 - categorical_accuracy: 0.9312 - val_loss: 0.0646 - val_categorical_accuracy: 0.9825\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0872 - categorical_accuracy: 0.9735 - val_loss: 0.0441 - val_categorical_accuracy: 0.9888\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0699 - categorical_accuracy: 0.9783 - val_loss: 0.0430 - val_categorical_accuracy: 0.9880\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0580 - categorical_accuracy: 0.9818 - val_loss: 0.0369 - val_categorical_accuracy: 0.9893\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0512 - categorical_accuracy: 0.9841 - val_loss: 0.0325 - val_categorical_accuracy: 0.9898\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0468 - categorical_accuracy: 0.9846 - val_loss: 0.0348 - val_categorical_accuracy: 0.9902\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0426 - categorical_accuracy: 0.9864 - val_loss: 0.0300 - val_categorical_accuracy: 0.9908\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0393 - categorical_accuracy: 0.9871 - val_loss: 0.0310 - val_categorical_accuracy: 0.9913\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0372 - categorical_accuracy: 0.9880 - val_loss: 0.0336 - val_categorical_accuracy: 0.9907\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0347 - categorical_accuracy: 0.9891 - val_loss: 0.0319 - val_categorical_accuracy: 0.9910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb70bbc4f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMgaUhopC2-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "84151c35-4545-457e-b7f2-d277b14d0d8e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_custom_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_custom_30 (Conv2D_cus multiple                  320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_custom_31 (Conv2D_cus multiple                  18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling multiple                  0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_custom_15 (Dense_custo multiple                  16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9cLSIg3bTMM",
        "colab_type": "text"
      },
      "source": [
        "# No model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63FMFCUtbWN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5852e3a8-60ad-424e-b597-6ead8908aa4c"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 10\n",
        "loss_func = keras.losses.CategoricalCrossentropy()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.003)\n",
        "\n",
        "(valX, valy) = (trainX[-10000:], trainy[-10000:])\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainy))\n",
        "\n",
        "# layers\n",
        "conv1 = Conv2D_custom(32, (3,3), activation='relu', input_shape = trainX.shape[1:])\n",
        "conv2 = Conv2D_custom(64, (3,3), activation='relu')\n",
        "dense = Dense_custom(10, activation='softmax')\n",
        "max1 = MaxPooling2D((2, 2))\n",
        "max2 = MaxPooling2D((2, 2))\n",
        "flat = Flatten()\n",
        "dropout = Dropout(0.5)\n",
        "\n",
        "for i in range(epochs):\n",
        "  print(\"Epoch: \", i)\n",
        "  epoch_loss = 0\n",
        "\n",
        "  train_dataset = train_dataset.shuffle(buffer_size=1024)\n",
        "  train_batched = train_dataset.batch(batch_size)\n",
        "\n",
        "  for step, (batchX, batchy) in enumerate(train_batched):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # we could have a model and pass the model to it,\n",
        "      # but instead we do that manually\n",
        "      x = conv1(batchX)\n",
        "      x = max1(x)\n",
        "      x = conv2(x)\n",
        "      x = max2(x)\n",
        "      x = flat(x)\n",
        "      x = dropout(x, training=True)\n",
        "      x = dense(x)\n",
        "      loss = loss_func(batchy, x)\n",
        "\n",
        "    trainable_vars = conv1.trainable_weights + conv2.trainable_weights + dense.trainable_weights\n",
        "    grads = tape.gradient(loss, trainable_vars)\n",
        "    opt.apply_gradients(zip(grads, trainable_vars))\n",
        "\n",
        "    epoch_loss += loss\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      print(\"\\tStep \", step, \":\\t loss = \", epoch_loss.numpy()/(step+1))\n",
        "  \n",
        "  # epoch ended, validate it\n",
        "  x = conv1(valX)\n",
        "  x = max1(x)\n",
        "  x = conv2(x)\n",
        "  x = max2(x)\n",
        "  x = flat(x)\n",
        "  x = dropout(x, training=False)\n",
        "  x = dense(x)\n",
        "  val_loss = loss_func(valy, x)\n",
        "  \n",
        "  print(\"Epoch \", i, \" ended.\\t\", \"loss = \", epoch_loss.numpy()/len(train_batched), \" ,\\tval_loss = \", val_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "\tStep  0 :\t loss =  2.304352283477783\n",
            "\tStep  200 :\t loss =  0.5012372904155978\n",
            "\tStep  400 :\t loss =  0.3453362291293251\n",
            "\tStep  600 :\t loss =  0.2741934067000962\n",
            "\tStep  800 :\t loss =  0.23165811641088288\n",
            "\tStep  1000 :\t loss =  0.21039996685443463\n",
            "\tStep  1200 :\t loss =  0.19224472109423787\n",
            "\tStep  1400 :\t loss =  0.17932576540961256\n",
            "\tStep  1600 :\t loss =  0.168869142454911\n",
            "\tStep  1800 :\t loss =  0.15904104517672474\n",
            "Epoch  0  ended.\t loss =  0.15519287109375  ,\tval_loss =  0.031682923\n",
            "Epoch:  1\n",
            "\tStep  0 :\t loss =  0.23685216903686523\n",
            "\tStep  200 :\t loss =  0.0838245277974143\n",
            "\tStep  400 :\t loss =  0.08508751695590126\n",
            "\tStep  600 :\t loss =  0.08139631553814931\n",
            "\tStep  800 :\t loss =  0.07697209466560355\n",
            "\tStep  1000 :\t loss =  0.07726785543558004\n",
            "\tStep  1200 :\t loss =  0.07650873722581443\n",
            "\tStep  1400 :\t loss =  0.0757356871033804\n",
            "\tStep  1600 :\t loss =  0.07607384133085766\n",
            "\tStep  1800 :\t loss =  0.07425128400888395\n",
            "Epoch  1  ended.\t loss =  0.07318690592447917  ,\tval_loss =  0.02739184\n",
            "Epoch:  2\n",
            "\tStep  0 :\t loss =  0.015461385250091553\n",
            "\tStep  200 :\t loss =  0.06768599552894707\n",
            "\tStep  400 :\t loss =  0.06607047578045852\n",
            "\tStep  600 :\t loss =  0.06552612761689502\n",
            "\tStep  800 :\t loss =  0.06435030438331481\n",
            "\tStep  1000 :\t loss =  0.06488931905496848\n",
            "\tStep  1200 :\t loss =  0.06462909021941352\n",
            "\tStep  1400 :\t loss =  0.06485417193808272\n",
            "\tStep  1600 :\t loss =  0.06411638801951174\n",
            "\tStep  1800 :\t loss =  0.06333175650707289\n",
            "Epoch  2  ended.\t loss =  0.06303512369791667  ,\tval_loss =  0.017453188\n",
            "Epoch:  3\n",
            "\tStep  0 :\t loss =  0.08105004578828812\n",
            "\tStep  200 :\t loss =  0.0590405962360439\n",
            "\tStep  400 :\t loss =  0.057703565183720384\n",
            "\tStep  600 :\t loss =  0.0571600585531276\n",
            "\tStep  800 :\t loss =  0.05530602863516552\n",
            "\tStep  1000 :\t loss =  0.05581003850275701\n",
            "\tStep  1200 :\t loss =  0.05506047003473668\n",
            "\tStep  1400 :\t loss =  0.056635555074012425\n",
            "\tStep  1600 :\t loss =  0.05673612824534715\n",
            "\tStep  1800 :\t loss =  0.05562665262598253\n",
            "Epoch  3  ended.\t loss =  0.05485467122395833  ,\tval_loss =  0.017626999\n",
            "Epoch:  4\n",
            "\tStep  0 :\t loss =  0.002603719010949135\n",
            "\tStep  200 :\t loss =  0.04939786711735512\n",
            "\tStep  400 :\t loss =  0.050541199948127724\n",
            "\tStep  600 :\t loss =  0.049246941945715474\n",
            "\tStep  800 :\t loss =  0.04846056153562929\n",
            "\tStep  1000 :\t loss =  0.0483016319922753\n",
            "\tStep  1200 :\t loss =  0.048866865934678455\n",
            "\tStep  1400 :\t loss =  0.050259112971412716\n",
            "\tStep  1600 :\t loss =  0.05055381684359873\n",
            "\tStep  1800 :\t loss =  0.04941027194906909\n",
            "Epoch  4  ended.\t loss =  0.04910872802734375  ,\tval_loss =  0.011917408\n",
            "Epoch:  5\n",
            "\tStep  0 :\t loss =  0.009247386828064919\n",
            "\tStep  200 :\t loss =  0.05028264439521144\n",
            "\tStep  400 :\t loss =  0.05071683774267943\n",
            "\tStep  600 :\t loss =  0.0456833260229939\n",
            "\tStep  800 :\t loss =  0.04566971282387494\n",
            "\tStep  1000 :\t loss =  0.046939381114490975\n",
            "\tStep  1200 :\t loss =  0.04727714405170984\n",
            "\tStep  1400 :\t loss =  0.04679242174256112\n",
            "\tStep  1600 :\t loss =  0.04742303928086938\n",
            "\tStep  1800 :\t loss =  0.046221278231386206\n",
            "Epoch  5  ended.\t loss =  0.04587159016927083  ,\tval_loss =  0.011895284\n",
            "Epoch:  6\n",
            "\tStep  0 :\t loss =  0.0015174184227362275\n",
            "\tStep  200 :\t loss =  0.04875624357764401\n",
            "\tStep  400 :\t loss =  0.045547713662620795\n",
            "\tStep  600 :\t loss =  0.047860635893912165\n",
            "\tStep  800 :\t loss =  0.04839953114179785\n",
            "\tStep  1000 :\t loss =  0.047730258175662225\n",
            "\tStep  1200 :\t loss =  0.04781145259402971\n",
            "\tStep  1400 :\t loss =  0.04740147178807827\n",
            "\tStep  1600 :\t loss =  0.04671309100025375\n",
            "\tStep  1800 :\t loss =  0.046049330910995626\n",
            "Epoch  6  ended.\t loss =  0.04611064046223958  ,\tval_loss =  0.012174975\n",
            "Epoch:  7\n",
            "\tStep  0 :\t loss =  0.06957735121250153\n",
            "\tStep  200 :\t loss =  0.04070365962697499\n",
            "\tStep  400 :\t loss =  0.04346282464310415\n",
            "\tStep  600 :\t loss =  0.03936363575660845\n",
            "\tStep  800 :\t loss =  0.04114085755842307\n",
            "\tStep  1000 :\t loss =  0.04059616073623642\n",
            "\tStep  1200 :\t loss =  0.04076153630519489\n",
            "\tStep  1400 :\t loss =  0.04113576481973674\n",
            "\tStep  1600 :\t loss =  0.040606230069219436\n",
            "\tStep  1800 :\t loss =  0.03972621072602894\n",
            "Epoch  7  ended.\t loss =  0.03931737467447917  ,\tval_loss =  0.012343783\n",
            "Epoch:  8\n",
            "\tStep  0 :\t loss =  0.01998886466026306\n",
            "\tStep  200 :\t loss =  0.03786351787510203\n",
            "\tStep  400 :\t loss =  0.03738288451311297\n",
            "\tStep  600 :\t loss =  0.03965381298604702\n",
            "\tStep  800 :\t loss =  0.04053000117955583\n",
            "\tStep  1000 :\t loss =  0.040795961698213895\n",
            "\tStep  1200 :\t loss =  0.04061672074113857\n",
            "\tStep  1400 :\t loss =  0.041979270352371076\n",
            "\tStep  1600 :\t loss =  0.04225938309735614\n",
            "\tStep  1800 :\t loss =  0.042559312887154706\n",
            "Epoch  8  ended.\t loss =  0.04206106363932292  ,\tval_loss =  0.009272478\n",
            "Epoch:  9\n",
            "\tStep  0 :\t loss =  0.06465799361467361\n",
            "\tStep  200 :\t loss =  0.045926402457317904\n",
            "\tStep  400 :\t loss =  0.04569303365122351\n",
            "\tStep  600 :\t loss =  0.04083158648549618\n",
            "\tStep  800 :\t loss =  0.03905001293853874\n",
            "\tStep  1000 :\t loss =  0.041150305535528925\n",
            "\tStep  1200 :\t loss =  0.040247177899032704\n",
            "\tStep  1400 :\t loss =  0.040774310681753544\n",
            "\tStep  1600 :\t loss =  0.04123378470121213\n",
            "\tStep  1800 :\t loss =  0.040994719357572616\n",
            "Epoch  9  ended.\t loss =  0.04117398681640625  ,\tval_loss =  0.008862505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGxx3ivqdt_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ec2b42b-64b0-4366-9e1c-89d24aeadf26"
      },
      "source": [
        "x = conv1(testX)\n",
        "x = max1(x)\n",
        "x = conv2(x)\n",
        "x = max2(x)\n",
        "x = flat(x)\n",
        "x = dropout(x, training=False)\n",
        "x = dense(x)\n",
        "\n",
        "test_loss = loss_func(testy, x)\n",
        "test_match = keras.metrics.categorical_accuracy(testy, x)\n",
        "test_acc = np.sum(test_match)/len(test_match)\n",
        "\n",
        "print(\"test_loss = \", test_loss.numpy(), \",\\ttest_accuracy = \", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_loss =  0.029017111 ,\ttest_accuracy =  0.9905\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
