{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST sequential learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5EOiQ2VoqhH3avcMoqeWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Black3rror/AI/blob/master/MNIST_sequential_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065zZBqri5bI",
        "colab_type": "text"
      },
      "source": [
        "# Goal\n",
        "\n",
        "Show the labels to the model sequentiall. Some how we want to find out how does the model learn, and what it learns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdkKnmV8cOyO",
        "colab_type": "text"
      },
      "source": [
        "# Progress\n",
        "\n",
        "- Show the first label and train the model, then add the next label and train the model, and go on this way. Compare the learning time of each label\n",
        "- Each time just show a fixed number of samples of the new label to the model\n",
        "- Each time show a fixed number of learnt samples, plus a fixed number of new samples of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5PD0zl0RFKz",
        "colab_type": "text"
      },
      "source": [
        "# Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iOv8AVrQ2Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np    # tf uses np so probabily we use np in our code\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation\n",
        "\n",
        "import matplotlib.pyplot as plt   # if u want to show imgs by pyplot\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime   # to organize TensorBoard files\n",
        "\n",
        "from keras.utils import to_categorical    # to change a number to one-hot key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esEFFbOoSLNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/   # Clear any logs from previous runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d388hJ2LoE2",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBRbrQPLsum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert len(tf.config.list_physical_devices('GPU')) > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exo-RHr6L0sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6d732868-1b55-4631-cc20-c4604d3a1d9b"
      },
      "source": [
        "(trainX, trainy), (testX, testy) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# one-hot\n",
        "trainy = to_categorical(trainy)\n",
        "testy = to_categorical(testy)\n",
        "\n",
        "# image should be in shape of (28, 28, 1) not (28, 28)\n",
        "trainX = np.expand_dims(trainX, -1)\n",
        "testX = np.expand_dims(testX, -1)\n",
        "\n",
        "# normalize\n",
        "trainX = trainX.astype(\"float32\")/255\n",
        "testX = testX.astype(\"float32\")/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpLj2kR5Tss4",
        "colab_type": "text"
      },
      "source": [
        "# Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO59Dq7IUTxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(conv1_units = 32, conv2_units = 64, dropout_rate = 0.5):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(conv1_units, (3,3), activation='relu', input_shape = trainX.shape[1:]))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(conv2_units, (3,3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gT0R9hDYAUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2cY-A2t8H7",
        "colab_type": "text"
      },
      "source": [
        "# Compile and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcaV3Qw_t-8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hMKiW-lvpCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f645101-22ae-46e2-fc02-50083aa7f3ac"
      },
      "source": [
        "(trainX_seq, trainy_seq) = (trainX[trainy[:, 0]==1], trainy[trainy[:, 0]==1])\n",
        "(trainX_combined, trainy_combined) = (trainX_seq[:6000], trainy_seq[:6000])\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"\\n\\nnew label: \", i)\n",
        "\n",
        "  if i != 0:\n",
        "    # 5400 of new label and 5400 of learnt ones\n",
        "    (trainX_new, trainy_new) = (trainX[trainy[:, i]==1], trainy[trainy[:, i]==1])\n",
        "\n",
        "    trainX_combined = np.concatenate((trainX_new[:5400], trainX_seq[:5400]), axis = 0)\n",
        "    trainy_combined = np.concatenate((trainy_new[:5400], trainy_seq[:5400]), axis = 0)\n",
        "    p = np.random.permutation(len(trainX_combined))\n",
        "    (trainX_combined, trainy_combined) = (np.array(trainX_combined)[p], np.array(trainy_combined)[p])\n",
        "    \n",
        "    trainX_seq = np.concatenate((trainX_seq, trainX[trainy[:, i]==1]), axis = 0)\n",
        "    trainy_seq = np.concatenate((trainy_seq, trainy[trainy[:, i]==1]), axis = 0)\n",
        "    p = np.random.permutation(len(trainX_seq))\n",
        "    (trainX_seq, trainy_seq) = (np.array(trainX_seq)[p], np.array(trainy_seq)[p])\n",
        "\n",
        "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-labels-\" + str(i)\n",
        "  tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "  model.fit(trainX_combined, trainy_combined, epochs=5, validation_data=(testX, testy), \n",
        "            callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "new label:  0\n",
            "Epoch 1/5\n",
            "  1/186 [..............................] - ETA: 0s - loss: 2.3546 - categorical_accuracy: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "  2/186 [..............................] - ETA: 4s - loss: 2.2471 - categorical_accuracy: 0.2188    WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0371s). Check your callbacks.\n",
            "186/186 [==============================] - 2s 10ms/step - loss: 0.0653 - categorical_accuracy: 0.9916 - val_loss: 18.2290 - val_categorical_accuracy: 0.0980\n",
            "Epoch 2/5\n",
            "186/186 [==============================] - 1s 8ms/step - loss: 1.1231e-08 - categorical_accuracy: 1.0000 - val_loss: 18.2297 - val_categorical_accuracy: 0.0980\n",
            "Epoch 3/5\n",
            "186/186 [==============================] - 1s 8ms/step - loss: 1.1412e-08 - categorical_accuracy: 1.0000 - val_loss: 18.2306 - val_categorical_accuracy: 0.0980\n",
            "Epoch 4/5\n",
            "186/186 [==============================] - 1s 7ms/step - loss: 9.4997e-09 - categorical_accuracy: 1.0000 - val_loss: 18.2317 - val_categorical_accuracy: 0.0980\n",
            "Epoch 5/5\n",
            "186/186 [==============================] - 1s 8ms/step - loss: 1.4974e-08 - categorical_accuracy: 1.0000 - val_loss: 18.2335 - val_categorical_accuracy: 0.0980\n",
            "\n",
            "\n",
            "new label:  1\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 4.6702 - categorical_accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0290s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0915 - categorical_accuracy: 0.9785 - val_loss: 15.0733 - val_categorical_accuracy: 0.2114\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0048 - categorical_accuracy: 0.9985 - val_loss: 15.5213 - val_categorical_accuracy: 0.2112\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0041 - categorical_accuracy: 0.9986 - val_loss: 14.4605 - val_categorical_accuracy: 0.2114\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0024 - categorical_accuracy: 0.9990 - val_loss: 14.7581 - val_categorical_accuracy: 0.2114\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0021 - categorical_accuracy: 0.9993 - val_loss: 14.3778 - val_categorical_accuracy: 0.2114\n",
            "\n",
            "\n",
            "new label:  2\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 10.7231 - categorical_accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.0216s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.1889 - categorical_accuracy: 0.9545 - val_loss: 13.2472 - val_categorical_accuracy: 0.3126\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0404 - categorical_accuracy: 0.9875 - val_loss: 12.4312 - val_categorical_accuracy: 0.3130\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0302 - categorical_accuracy: 0.9908 - val_loss: 12.9604 - val_categorical_accuracy: 0.3135\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0249 - categorical_accuracy: 0.9921 - val_loss: 13.1462 - val_categorical_accuracy: 0.3137\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0195 - categorical_accuracy: 0.9942 - val_loss: 13.4884 - val_categorical_accuracy: 0.3141\n",
            "\n",
            "\n",
            "new label:  3\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 11.0005 - categorical_accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0057s vs `on_train_batch_end` time: 0.0231s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.3414 - categorical_accuracy: 0.9222 - val_loss: 9.5708 - val_categorical_accuracy: 0.4099\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0734 - categorical_accuracy: 0.9767 - val_loss: 9.8511 - val_categorical_accuracy: 0.4120\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0603 - categorical_accuracy: 0.9788 - val_loss: 10.3703 - val_categorical_accuracy: 0.4123\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0546 - categorical_accuracy: 0.9817 - val_loss: 10.0689 - val_categorical_accuracy: 0.4128\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0502 - categorical_accuracy: 0.9835 - val_loss: 10.1732 - val_categorical_accuracy: 0.4123\n",
            "\n",
            "\n",
            "new label:  4\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 8.8210 - categorical_accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0061s vs `on_train_batch_end` time: 0.0280s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.4249 - categorical_accuracy: 0.8931 - val_loss: 12.2005 - val_categorical_accuracy: 0.5059\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.1206 - categorical_accuracy: 0.9670 - val_loss: 12.5838 - val_categorical_accuracy: 0.5054\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.1068 - categorical_accuracy: 0.9697 - val_loss: 12.1202 - val_categorical_accuracy: 0.5071\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0951 - categorical_accuracy: 0.9735 - val_loss: 12.9647 - val_categorical_accuracy: 0.5078\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.0896 - categorical_accuracy: 0.9743 - val_loss: 12.8206 - val_categorical_accuracy: 0.5091\n",
            "\n",
            "\n",
            "new label:  5\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 15.4723 - categorical_accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_end` time: 0.0235s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 1.1504 - categorical_accuracy: 0.6581 - val_loss: 8.3847 - val_categorical_accuracy: 0.5765\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2853 - categorical_accuracy: 0.9109 - val_loss: 9.7054 - val_categorical_accuracy: 0.5849\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2316 - categorical_accuracy: 0.9305 - val_loss: 11.4148 - val_categorical_accuracy: 0.5867\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2035 - categorical_accuracy: 0.9387 - val_loss: 10.5292 - val_categorical_accuracy: 0.5868\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.1618 - categorical_accuracy: 0.9505 - val_loss: 10.1557 - val_categorical_accuracy: 0.5927\n",
            "\n",
            "\n",
            "new label:  6\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 13.3469 - categorical_accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_end` time: 0.0240s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.6682 - categorical_accuracy: 0.8036 - val_loss: 5.2377 - val_categorical_accuracy: 0.6642\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2992 - categorical_accuracy: 0.9014 - val_loss: 5.6398 - val_categorical_accuracy: 0.6695\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2499 - categorical_accuracy: 0.9213 - val_loss: 5.8846 - val_categorical_accuracy: 0.6763\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2175 - categorical_accuracy: 0.9322 - val_loss: 5.8125 - val_categorical_accuracy: 0.6752\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.1897 - categorical_accuracy: 0.9406 - val_loss: 6.0596 - val_categorical_accuracy: 0.6808\n",
            "\n",
            "\n",
            "new label:  7\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 7.5660 - categorical_accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0222s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 1.4695 - categorical_accuracy: 0.5243 - val_loss: 4.3220 - val_categorical_accuracy: 0.7256\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.5517 - categorical_accuracy: 0.8365 - val_loss: 5.1236 - val_categorical_accuracy: 0.7540\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.4173 - categorical_accuracy: 0.8805 - val_loss: 5.0704 - val_categorical_accuracy: 0.7614\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.3091 - categorical_accuracy: 0.9096 - val_loss: 5.6807 - val_categorical_accuracy: 0.7661\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.2833 - categorical_accuracy: 0.9192 - val_loss: 6.3197 - val_categorical_accuracy: 0.7702\n",
            "\n",
            "\n",
            "new label:  8\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 14.1151 - categorical_accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 2.0940 - categorical_accuracy: 0.2870 - val_loss: 2.8205 - val_categorical_accuracy: 0.6189\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.9335 - categorical_accuracy: 0.7255 - val_loss: 3.2927 - val_categorical_accuracy: 0.7587\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.6681 - categorical_accuracy: 0.8026 - val_loss: 3.2300 - val_categorical_accuracy: 0.7973\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.5348 - categorical_accuracy: 0.8382 - val_loss: 3.3398 - val_categorical_accuracy: 0.8298\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 0.4403 - categorical_accuracy: 0.8687 - val_loss: 3.3859 - val_categorical_accuracy: 0.8330\n",
            "\n",
            "\n",
            "new label:  9\n",
            "Epoch 1/5\n",
            "  1/338 [..............................] - ETA: 0s - loss: 14.1399 - categorical_accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0282s). Check your callbacks.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 2.7681 - categorical_accuracy: 0.0740 - val_loss: 2.3987 - val_categorical_accuracy: 0.1135\n",
            "Epoch 2/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 2.1844 - categorical_accuracy: 0.1627 - val_loss: 2.3869 - val_categorical_accuracy: 0.1009\n",
            "Epoch 3/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 1.9857 - categorical_accuracy: 0.5000 - val_loss: 2.4295 - val_categorical_accuracy: 0.1009\n",
            "Epoch 4/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 1.8812 - categorical_accuracy: 0.5000 - val_loss: 2.4910 - val_categorical_accuracy: 0.1009\n",
            "Epoch 5/5\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 1.8301 - categorical_accuracy: 0.5000 - val_loss: 2.5489 - val_categorical_accuracy: 0.1009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H83lv7TkZj45",
        "colab_type": "text"
      },
      "source": [
        "# TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JBtWAuPwZYBN",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/fit    # to run the TensorBoard in the notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxvqwZNIy9Ok",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c_iV4bD_2eq",
        "colab_type": "text"
      },
      "source": [
        "Show the first label and train the model, then add the next label and train the model, and go on this way. Compare the learning time of each label\\\n",
        "Result: Done the task above, resuls shows that training gets a bit harder for later labels (opposite of what we thought). The reason is that we didn't mention that when we want to train the model with later labels, new label has less impact in the training batch so it takes more iterations for the model to learn the new label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_O9n3_E_2Lb",
        "colab_type": "text"
      },
      "source": [
        "Each time just show a fixed number of samples of the new label to the model\\\n",
        "Result: Results were so interesting. On training data, results got worse for later labels. The reason can be, at first weights and biases were around zero but after learning each label, they will get further from zero, so the next label has more trouble to tune them. So I think by expanding the model, this phenomenon may diminish a bit.\\\n",
        "If we look at the validation data to see if the model just learns to output one label or it learns the concept (which will be weired in this case) or even to see how much the model forgets its past labels, we will see interesting things. It doesn't learn the concept and even forgets the past labels easily except for second label (label 1). I guess if we narrow the model (less neurons in each layer) and/or make it deeper (more layers), the model will not act this special behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOfAIlabrypw",
        "colab_type": "text"
      },
      "source": [
        "Each time show a fixed number of learnt samples, plus a fixed number of new samples of the model\n",
        "Result: The result shows that learning later ones takes more time (opposite of what we thought). Its weired that for learning new labels, the model starts from around zero accuracy! And more weired one is that for label 9, finally the model learns to output just 9 (with 40% probability) and riches the accuracy of around 50%. Currently I have no idea of the reason and it can be investigated more to see how the model works."
      ]
    }
  ]
}
